# GFS 学习理解笔记
## 设计
### 目标
- 直接面向业务大量客户端直接提供数据处理。
- 设计基于具体应用的负载类型以及当前甚至未来技术环境的观察
- 大数据服务的基础存储平台
- 最大容量是1000台机器、数千块磁盘、300T的存储，同时提供数百个客户端并发访问。
- 设计观点来自真实世界的测量数据和性能测试的结合

### 和传统分布式文件系统具有相同目标
- 性能
- 可扩展性
- 可靠性
- 可用性

### 区别
- 由于程序bug、操作系统bug、认为错误、管理物理设备激增、运营环境等因素，使得故障是常态化的。所以经常性的监控、错误检测、容错和自动恢复必须集成在系统中。
- 文件巨大，GB 单位的文件是很普通的。数据集合总量达到 TB 级别，包含数十亿的对象，处理十亿KB 级别的文件。这样的条件下对于 IO 操作和块大小就需要重新定义。
- 文件更新模式是追加模式，随机写操作不存在，文件只读和顺读，大量数据都具有这样的特点，那么当缓存数据在客户端失效后，append操作就成为性能优化和原子性的关键。
- 应用程序和文件系统api的协同设计，增加了整个系统的灵活性。比如放松了 GFS 的一致性来简化文件系统，同时也没给程序带来很大负担。通过提供原子性的append操作，多个客户端就可以对同一个文件并行的进行append操作而不需要彼此间进行额外的同步操作

### 假设
- 软件、硬件、网络、人为因素导致故障是常态，所以检测、监控、容错并自动恢复是必须有的功能
- 对大文件的存储是普遍现象，并且需要进行有效管理。而小文件可以支持，但是不需要特殊优化。
- 工作负载主要由两种类型的读组成
	- 大的流式读取

		单个操作读取数百kb或更大。来源于同一客户端的连续读取，并读完文件的一个连续区域。
	- 小的随机读取
	 
	 	在某个任意的偏移位置读取几kb的数据。具有性能意识的程序会把这些小随机读取按批次，排序使得读取可以稳步穿越整个文件而不是来回读取。
- 工作负载由很大对文件 append 操作，操作大小类似，写完就很少修改，在文件内部随机写操作支持，但是性能不要求。
- 系统对于定义良好的多个客户端对同一个文件进行 append 操作必须提供有效实现。文件通常会被用来做生产和消费者队列使用，并会多路归并。数百个生产者操作一个文件 append，所以原子性是必须的。之后再被消费者并行读取。
- 高的持续带宽比低延时更重要，大部分目标都希望得到高速批量数据处理，对于单个读写没有严格的响应需求。

### 接口
提供了一个熟悉的文件系统接口，没有实现标准类 POSIX 的 API。文件通过目录进行层次化组织，通过路径来标示文件。

支持操作

- create
- delete
- open
- close
- read
- write
- 快照

	快照操作以很低的开销创建文件或者目录拷贝。
- append 操作

	append 操作允许多个客户向一个文件并发操作写，并且保证原子性。客户端不需要额外的锁。

### 架构
集群由一个 master 和多个 chunkserver 组成。可以响应多个 client 端的并发请求。

![](./pic/gfs1.png)

1. 利用所有资源

	因为无状态应用堆低可靠性是可以接受的，所以可以在任何的机器上运行一个 chunkserver 和 client
- 文件被划分为固定大小的 chunk，每个 chunk 是由 master 分配一个不可变全局唯一的 64 bit 句柄标示。 chunkserver 将 chunk 作为文件存储在本地，对 chunk 数据的读写通过 chunk 的 handle 和字节边界表示。为了高可用，默认存储备份3份到不同的 chunkserver 上。
- master 维护所有文件系统的元数据和控制系统的一些活动，如 chunk 租赁管理，垃圾回收，chunkserver 之间的 chunk 迁移等等。master 和 chunkserver 之间通过心跳进行周期通讯，发送指令和收集状态。
- 应用程序通过连接 GFS 的客户端 API 和 master 以及 chunkserver 进行读写通讯。客户端获取元数据是通过 master，而数据是直接从 chunkserver 获取。
- 客户端和 chunkserver 都不会进行文件数据缓存。大部分的应用需要直接读取整个大文件或工作集，但它们太大根本无法缓存。没有 cache 简化了整个系统，因为不需要考虑一致性问题。而客户端会缓存元数据。 chunkserver 不进行缓存是因为 chunk 作为本地文件存储， linux 会自己缓存经常访问的数据。
### 单 master
- 只有一个 master 简化了系统设计，这样可以很好的利用全局信息对 chunk 的安放和备份进行更好的判断。
- master 被设计成最小化参与读写操作，让它本身不成为瓶颈。 client 永远不会通过 master 读取文件数据。 client 会在现有时间内缓存元数据，这样可以直接和 chunkserver 交互。

- 交互过程

	1. 通过固定大小的chunk，客户端将应用程序中标识的文件名和 offset 转换为 chunk 的 index
	2. 给 master 发送一个包含文件名和 chunk index 的请求
	3. master 返回相应的 chunk 的 handle 和所有备份的位置
	4. 客户端以文件名和 chunk index 为 key 将这条信息进行缓存
	5. 然后客户端给最近的一个 chunkserver 备份发送一个请求(该请求标示了 chunk 的 handle 以及在那个 chunk 内的字节边界)
- 客户端在一个请求中查询多个 chunk 信息，master 会将多个请求打包一起返回，增加效率。

### 2.5 chunk大小
Chunk 64 MB 的大小是一个关键的设计参数。远大于现有文件系统的块。每个 chunk 的副本作为普通 linux 文件存储在 chunkserver 上，如果需要才扩展。这样的空间分配避免了碎片造成的空间浪费。

#### 2.5.1 大 chunk 的优势
- 减少了 client 和 server 之间的交互，在相同 chunk 上读写只需要一个初始化请求就可以从 master 上得到 chunk 元数据。应用的大部分需求是顺序的读写大文件。客户端也很容易的缓存一个几TB工作集的所有 chunk 元数据
- 因为 chunk 大，所以客户端很容易在一个 chunk 上做很多操作，这样可以在一个 chunkserver 的 tcp 保持更长时间来减少网络开销。
- 降低了存储在master上的元数据大小。允许将元数据存放在内存中

#### 2.5.2 大 chunk 的缺点
- 小文件导致 chunk 分片少，这样多 client 都同时访问，会导致在 chunkserver 形成热点。(在实际中，热点没有成为主要考虑因素是因为应用处理大部分都是大文件)
- 当 GFS 第一次使用在一个批处理队列时，热点出现，一个可执行文件写到 chunk 文件到 GFS,然后同时数百的机器都开始读它。导致 chunkserver 超载。通过更高级别的备份存储这样的文件来减少压力。长期方案时客户端进行 p2p 传输数据。

### 2.6 元数据
master 存储三个主要类型的元数据

- 文件和chunk名字空间
- 文件到chunk的映射信息
- 每个chunk的备份的位置

所有元数据都存储在内存中，前2个类型还通过更新操作日志保存在本地硬盘和备份在远程机器来保持持久化。log 允许简单可靠的更新 master 状态而不用担心当机时候带来的不一致。 master 并没有永久保存 chunk 的位置信息。而是 master 在启动或者某个 chunkserver 加入集群时，向每个 chunkserver 询问 chunk 信息。

#### 2.6.1 内存数据结构
因为是内存，所以可以简单有效的对后台整个状态进行周期性扫描，来实现chunk垃圾回收。垃圾可能产生的点

- chunkserver 出现失败时进行的重复制
- 为了平衡负载和磁盘空间在chunkserver间的 chunk 迁移

全内存元数据存储带来的是整个系统的荣来那个取决于 master 的内存大小。但因为每个 64MB 的 chunk 元数据少于 64 byte。而大部分文件又包含多个 chunk ，而每个文件的名字空间数据又小于 64 byte，所以存储文件名字的时候会进行前缀压缩。

如果想支持更大的文件系统，只需要向 master 增加内存即可
#### 2.6.2 chunk location
master 没有一个用永久存储，对于一个 chunk 都是那些 chunkserver 保存了原数据副本。 master 在启动的时候把 chunkserver 的信息拉过来。Master 保证它自己的数据是更新过的并控制 chunk 的存放的，以及通过周期性的心跳信息来监控 chunkserver。

通过 master 启动加载数据,可以避免当 chunkserver 在加入或者离开集群，改名，失败，重启等待时需要的 master 与chunkserver 间的同步。

理解设计的另一个方式是 chunkserver 对于自己有还是没有某个 chunk 具有最终的发言权。master 上维护这些信息没有意义，因为如果发生 chunkserver 上的错误，chunk 丢失， 或者将 chunkserver 重命名。
#### 2.6.3 操作日志
操作日志包含了关键元数据改变的历史记录。它是GFS的核心。它是元数据的唯一一致性记录，也定义了那些并发操作的逻辑上的时间表。文件和 chunk 的版本都是唯一和持久化，都由它们创建时的逻辑时间来标识的。

在任何元数据变更被持久化之前不应当被客户端看到，否则，我们将丢失整个文件系统或者最近的客户端操作，即使chunckserver 保存了它们。

因此需要将它备份在多个机器上，对于一个客户端操作只有当该操作对应的日志记录被同时刷新到本地和远程的磁盘上时才会发出响应。Master 将几个操作日志捆在一块刷新，从而降低刷新和复制对于整个系统吞吐率的影响。

Master 通过重新执行操作日志来恢复它的文件系统。为了最小化启动时间，必须将日志保持在很小的规模。当日志增长超过一定的大小后，Master 给它的状态设置检查点，它可以通过从本地磁盘加载最新的检查点进行恢复，然后重新执行那些在检查点之后的日志记录。

检查点保存了一个压缩的类B树的结构，不需要额外的解析就可以直接映射到内存用于名字空间查找。大大提高了恢复的速度和可用性。

建立检查点会花费时间，master 的内部状态的结构设计使得一个新的检查点可以不需要延时那些新请求就可以被创建。 Master 会启动一个新的线程切换到一个新的日志文件然后创建新的检查点，新的检查点包含在切换之前的所有变更。对于一个包含几百万个文件的集群大概需要几分钟就可以完成。结束后，它将会被写回本地和远程的磁盘。

恢复只需要最新的检查点和后来的日志文件。更老的检查点和日志文件可以自由的删除，当然会保存了一些来应对某些突发情况。

在创建检查点的时候发生的失败不会影响系统的正确性，因为恢复代码会检测和跳过失败的检查点。
#### 2.7一致性模型
GFS使用的一个放松的一致性模型不但很好的支持了高度分布式的应用，而且实现起来也相对简单和有效率。
#### 2.7.1 GFS提供的保证
文件名字空间的改变(比如文件创建)是原子性的。只由master进行处理，Master 的操作日志定义了这些操作的全局性的排序。当数据变更后，文件区域的状态取决于变更的类型，如变更是否成功以及是否是并发进行的。

![](./pic/gfs2.png)

如果客户端无论从哪个副本读取数据总是相同的，那么我们就说文件区域是一致的。如果文件数据变更后是一致的，同时客户端可以看到它所有的变更，那么就说文件区是已定义。

当一个变更成功后，且没有受到其他并发写者的影响，那么被影响的区域就是定义良好的(肯定是一致性的)，所有的客户端将会看到所做的变更。

并发的成功的变更，会使区域进入未定义的状态但是还是一致的：所有的客户端可以看到一致的数据，但是它可能无法看到所有的变更(如果变更是针对相同的数据写这样有的变更就会被新的变更所覆盖，这样用户就无法看到最先的变更了，同时发生在跨 chunk 的操作会被拆分成两个操作，这样这个操作的一部分可能会被其他操作覆盖，而另一部分则保留下来，如3.1节末尾所述)。

通常它看到的是多个变更组合后的结果。一个失败的变更会使区域进入非一致的状态(因此也是未定义的状态)：不同的客户端在不同的访问中可能看到不同的数据。

应用程序不需要进一步区分未定义区域的各种不同的类型。数据变更可能是两种操作：写操作或者记录 append。

- 写操作会使数据在应用程序指定的偏移位置写入。
- 记录 append 操作会使数据原子性的 append，如果是并发性的话则至少会被 append 一次，但是偏移位置是由 GFS 决定的(然而，通常的理解可能是在客户端想写入的那个文件的尾部)。偏移位置会被返回给客户端，同时标记包含这条记录的那个定义良好的文件区域的起始位置。另外GFS可能会在它们之间插入一些padding或者记录的副本。它们会占据那些被认为是不一致的区域，通常它们比用户数据小的多。

一系列成功的变更后，变更文件的区域被保证是已定义的，同时包含了最后一次变更的数据写入。GFS通过两种方式来实现这种结果

- 将这些变更以相同的操作顺序应用在该 chunk 的所有的副本上
- 使用 chunk 的版本号来检测那些老旧的副本可能是由于它的 chunkserver 挂掉了而丢失了一些变更

陈旧的副本永远都不会参与变更或者返回给那些 client 的请求，它们会优先参与垃圾回收。但因为客户端会缓存 chunk 的位置，在更新之前可能会读到陈旧的副本。时间窗口由缓存值的超时设置和文件的下一次打开而限制，文件的打开会清除缓存中该文件相关的 chunk 信息。由于大部分操作都是记录的 append，因此一个陈旧副本通常会返回一个过早结束的 chunk 而不是过时的数据。当读取者重试并与 master 联系时，它会立即得到当前的 chunk 位置。

成功的变更很久之后，组件失败仍有可能破坏或者污染数据。GFS通过周期性的在 master 和所有 chunkserver 间握手找到那些失败的 chunkserver，同时通过校验和(5.2节)来检测数据的污染。一旦发现问题，会尽快的利用正确的副本恢复(4.3节)。只有所有副本在GFS做出反应之前全部丢失，这个块才会不可逆转的丢失，而通常GFS的反应是在几分钟内的。即使在这种情况下，块不可用，而不是被污染：应用程序会收到清晰的错误信息而不是被污染的数据。
#### 2.7.2 对于应用程序的影响
GFS应用程序可以通过使用简单的技术来适应这种放松的一致性模型，这些技术已经为其他目的所需要：

- 依赖与append操作而不是覆盖
- 检查点
- 写时自我验证
- 自己标识记录

实际中，所有的应用程序都是通过append改变文件。

典型应用中

- 一个写操作者会从头至尾生成一个文件。当写完所有数据后它自动的将文件重命名为一个永久性的名称
- 或者通过周期性的检查点检查已经有多少数据被成功写入了。检查点可能会设置应用级的校验和。
- 读取者仅验证和处理最后一个检查点之前的文件区域，这些区域处于已定义的状态。

无论什么样的并发和一致性要求，这个方法都工作的很好。Append 操作比随机写对于应用程序的失败处理起来总是要更加有效和富有弹性。检查点允许写操作者增量性的重启(不需要重新从头写)，允许读取者可以处理那些已经成功写入的数据，虽然在应该程序的看来仍然是不完全的。

另一种典型的应用中

- 很多写者同时向一个文件 append 为了归并文件或者是作为一个生产者消费者队列。
- 记录的 append 的 append-at-least-once 语义保证了每个写者的输出。
- 读取者这样处理偶然的 padding 和重复数据。写者为每条记录准备一些额外信息比如校验和，这样它的合法性就可以验证。
- 如果不能容忍重复的数据(比如它们可能触发非幂等操作)，可以通过在记录中使用唯一标识符来过滤它们，很多时候都需要这些标识符命名相应的应用程序实体，比如网页文档。
- 这些用于record输入输出的功能函数是以库的形式被我们的应用程序共享的，同时应用于google其他的文件接口实现。所以，相同系列的记录，加上一些罕见的重复，总是直接被分发给记录读取者。

在以上的描述中，存在一个基本的假定：数据是以record形式存储的，而且通常这些record都是可以重复的，比如一个网页文档，这对于数百亿的网页文档，少数多余很正常，也就是说这些数据通常是文本，而不是二进制，所以我们才可以在append或者写时用记录的副本来覆盖非一致的区域，所以提供了append的append-at-least-once语义，因为append二次也是可以的。如果要保证唯一性，可以在应用层增加逻辑。

## 3.系统交互
gfs 以尽量最小化 master 在所有操作中的参与度来设计系统的。 
### 3.1租约和变更顺序
一个变更是指一个改变 chunk 的内容或者元信息的操作，比如写操作或者 append 操作。每个变更都需要在所有的副本上执行。我们使用租约来保持多个副本间变更顺序的一致性。

- Master 授权给其中的一个副本一个该 chunk 的租约，我们把它叫做主副本。
- 这个主副本会针对该 chunk 的所有变更的选择一个执行顺序，然后所有的副本根据这个顺序执行变更。
- 因此，全局的变更顺序首先是由 master 选择的租约授权顺序来确定的(可能有多个chunk需要进行修改)，而同一个租约内的变更顺序则是由那个主副本来定义的。 

租约机制是为了最小化 master 的管理开销而设计的。

- 一个租约有一个初始化为60s的超时时间设置。
- 只要这个chunk正在变更，那个主副本就可以向 master 请求延长租约。
- 这些请求和授权通常是与 master 和 chunkserver 间的心跳信息一起发送的。
- 有时候 master 可能想在租约过期前撤销它(比如，master 可能想使对一个正在重命名的文件的变更无效)。即使 master 无法与主副本进行通信，它也可以在旧的租约过期后安全的将租约授权给另一个新的副本。

![](./pic/gfs3.png)

1. client 向 master 询问那个 chunkserver 获取了当前 chunk 的租约以及其他副本所在的位置。如果没有人得到租约，master 将租约授权给它选定的一个副本。
- master 返回该主副本的标识符以及其他副本的位置。Client 为未来的变更缓存这个数据。只有当主副本没有响应或者租约到期时它才需要再与 master 联系。
- client 将数据推送给所有的副本，client 可以以任意的顺序进行推送。每个 chunkserver 会将数据存放在内部的 LRU buffer 里，直到数据被使用或者过期。通过将控制流与数据流分离，可以通过将昂贵的数据流基于网络拓扑进行调度来提高性能，而不用考虑哪个 chunkserver 是主副本。
- 一旦所有的副本接收到了数据，client 发送一个写请求给主副本，这个请求标识了先前推送给所有副本的数据。主副本会给它收到的所有变更(可能来自多个client)安排一个连续的序列号来进行必需的串行化。它将这些变更根据序列号应用在本地副本上。
- 主副本将写请求发送给所有的次副本，每个次副本以与主副本相同的串行化顺序应用这些变更。
- 所有的次副本完成操作后向主副本返回应答
- 主副本向client返回应答。任何副本碰到的错误都会返回给client。出现错误时，该写操作可能已经在主副本以及一部分次副本上执行成功。(如果主副本失败，那么它不会安排一个序列号并且发送给其他人)。客户端请求将会被认为是失败的，被修改的区域将会处在非一致状态下。我们的客户端代码会通过重试变更来处理这样的错误。它会首先在3-7步骤间进行一些尝试后在重新从头重试这个写操作。

如果应用程序的一个写操作跨越了 chunk 的边界时，GFS client 代码会将它转化为多个写操作。它们都会遵循上面的控制流程，但是可能会被其他 client 的操作插入或者覆盖。因此共享的文件区域可能会包含来自不同 client 的片段，虽然这些副本是一致的，因为所有的操作都按照相同的顺序在所有副本上执行成功了。但是文件区域会处在一种一致但是未定义的状态，正如2.7节描述的那样。

### 3.2数据流
为了更有效的使用网络将数据流和控制流分离。控制流从 client 到达主副本，然后到达其他的所有次副本，而数据则是线性地通过一个仔细选择的 chunkserver 链像流水线那样推送过去的。目标是最小化数据推送的延时，通过

- 充分利用每个机器的网络带宽
- 避免网络瓶颈和高延时链路

为了充分利用每个机器的网络带宽，数据通过 chunkserver 链线性的推送过去而不是以其他的拓扑进行分布比如树型。因此每个机器的带宽可以全部用来发送数据而不是为多个接受者进行切分。

为了尽可能的避免网络瓶颈和高延时链路，每个机器向网络中还没有收到该数据的最近的那个机器推送数据。

假设 client 将数据推送给S1-S4

- 它会首先将数据推送给最近的 chunkserver 假设是S1
- S1推送给最近的，假设S2
- S2推送给S3、S4中离他最近的那个

我们网络拓扑足够简单，以至于距离可以通过 IP 地址估计出来。

最后为了最小化延时，我们通过将 TCP 数据传输进行流水化。一旦一个 chunkserver 收到数据，它就开始立即往下发送数据。流水线对我们来说尤其有用，因为我们使用了一个全双工链路的交换网络。立即发送数据并不会降低数据接受速率。如果没有网络拥塞，向 R 个副本传输 B 字节的数据理想的时间耗费是 `B/T+RL` ,T代表网络吞吐率，L是机器间的网络延时。我们的网络连接是100Mbps(T),L远远低于1ms，因此1MB的数据理想情况下需要80ms就可以完成。

### 3.3原子性的记录 append
GFS 提供一个原子性的 append 操作叫做 record append (注意这与传统的 append 操作也是不同的)。传统写操作中，用户指定数据需要写便宜的位置。对于相同区域的并行写操作是不可串行的：该区域的末尾可能包含来自多个 client 的数据片段。但在一个 record append 操作中，client 唯一需要说明的只有数据。GFS会将它至少原子性地 append 到文件中一次，append 的位置是由GFS选定的，同时会将这个位置返回给 client。这很类似于 unix 文件打开模式中的 O_APPEND，当多个写操作并发时不会产生竞争条件。

Record append 在分布式应用中被大量的使用。很多在不同机器的 client 并发地向同一个文件 append。如果使用传统的写操作，client 将需要进行复杂而又昂贵的同步化操作，比如通过一个分布式锁管理器。在我们的工作负载中，这样的文件通常作为

- 一个多生产者/单消费者队列
- 保存来自多个不同 client 的归并结果。

Record append 是一种类型的变更操作，除了一点在主副本上的额外的逻辑外依然遵循 3.1 节的控制流。

- Client 将所有的数据推送给所有副本后，它向主副本发送请求。
- 主副本检查将该记录 append 到该 chunk 是否会导致该 chunk 超过它的最大值(64MB)。
	- 超过了
		- 将该 chunk 填充到最大值
		- 告诉次副本做同样的工作
		- 然后告诉客户端该操作应该在下一个 trunk 上重试。(append 的 Record 大小需要控制在最大 trunk 大小的四分之一以内，这样可以保证最坏情况下的碎片可以保持在一个可以接受的水平上 )。
	- 如果记录可以没有超过最大尺寸，就按照普通情况处理
		- 主副本将数据append到它的副本上
		- 告诉次副本将数据写在相同的偏移位置上
		- 最后向 client 返回成功应答。

如果 record append 在任何一个副本上失败，client 就会重试这个操作。这样，相同 chunk 的多个副本就可能包含不同的数据，这些数据可能包含了相同记录的整个或者部分的重复值。GFS并不保证所有的副本在位级别上的一致性，它只保证数据作为一个原子单元最少写入一次。这个属性是由如下的简单观察推导出来的，当操作报告成功时，数据肯定被写入到某个trunk的所有副本的相同偏移位置上。此后，所有的副本至少达到了记录尾部的大小，因此未来的记录将会被放置在更高的便宜位置，或者是另一个不同的chunk，即使另一个副本变成了主副本。在我们的一致性保证里，record append 操作成功后写下的数据区域是已定义的(肯定是一致的)，然而介于其间的数据则是不一致的(因此也是未定义的)。我们的应用程序可以处理这样的不一致区域，正如我们在2.7.2里讨论的那样。 

### 3.4快照
快照操作可以非常快速的保存文件或者目录树的一个拷贝，同时可以最小化对于正在执行的变更操作的中断。用户经常用它来：

- 创建大数据集的分支拷贝
- 拷贝的拷贝……
- 用来创建检查点，以实验将要提交的拷贝或者回滚到更早的状态

像AFS，我们使用标准的写时拷贝技术来实现快照。当 master 收到一个快照请求时，

- 它首先撤销将要进行快照的那些文件对应的 chunk 的所有已发出的租约。
- 这就使得对于这些 chunk 的后续写操作需要与 master 交互来得到租约持有者。
- 这就首先给master 一个机会创建该 chunk 的新的拷贝。

当这些租约被撤销或者过期后，master 将这些操作以日志形式写入磁盘。然后复制该文件或者目录树的元数据，然后将这些日志记录应用到内存中的复制后的状态上，新创建的快照文件与源文件一样指向相同的 chunk。

当 client 在快照生效后第一次对一个 chunk C 进行写入时

- 它会发送请求给master找到当前租约拥有者。
- Master 注意到对于 chunk C 的引用计数大于1。它延迟回复客户端的请求，选择一个新的 chunk handle C 。
- 然后让每个拥有 C 的那些 chunkserver 创建一个新的叫做 C 的 chunk。

通过在相同的 chunkserver 上根据原始的 chunk 创建新 chunk，就保证了数据拷贝是本地地，而不是通过网络(我们的硬盘比100Mbps 网络快大概三倍)。这样，对于任何 chunk的请求处理都没有什么不同：master为新才chunk C 的副本中的一个授权租约，然后返回给client，这样它就可以正常的写这个chunk了，client不需要知道该chunk实际上是从一个现有的chunk创建出来的。

## 4.master 操作
Master 执行所有的名字空间操作。此外，它还管理整个系统的 chunk 备份：

- 决定如何放置、创建新的 chunk 和相应的副本
- 协调整个系统的活动保证 chunk 都是完整备份的
- 在 chunkserver 间进行负载平衡
- 回收没有使用的存储空间。

我们现在讨论这些主题。
### 4.1 名字空间管理和锁
很多 master 操作都需要花费很长时间：比如，一个快照操作要撤销该快照所包含的 chunk 的所有租约。我们并不想耽误其他运行中的 master 操作，因此我们允许多个操作同时是活动的，通过在名字空间区域使用锁来保证正确的串行化。

不像传统的文件系统，GFS 的目录并没有一种数据结构用来列出该目录下所有文件，而且也不支持文件或者目录别名(像unix的硬链接或者软连接那样)。GFS在逻辑上通过一个路径全称到元数据映射的查找表来表示它的名字空间。通过采用前缀压缩，这个表可以有效地在内存中表示。名字空间树中的每个节点(要么是文件的绝对路径名称要么是目录的)具有一个相关联的读写锁。

每个 master 操作前，需要获得一个锁的集合。比如它想操作 `/d1/d2…/dn/leaf`，那么它需要获得 `/d1,/d1/d2……/d1/d2…/dn` 这些目录的读锁，然后才能得到路径 `/d1/d2…/dn/leaf` 的读锁或者写锁。Leaf 可能是个文件或者目录，这取决于具体的操作。

当为 `/home/user` 创建快照 `/save/user` 时，锁机制如何防止文件 `/home/user/foo` 被创建。

- 快照操作需要获得在 `/home /save` 上的读锁，以及 `/home/user` 和 `/save/user` 上的写锁。
- 文件创建需要获得在 `/home` 和 `/home/user` 上的读锁，以及在 `/home/user/foo` 上的写锁。
- 这两个操作将会被正确的串行化，因为它们试图获取在 `/home/user` 上的相冲突的锁。

文件创建并不需要父目录的写锁，因为实际上这里并没有”目录”或者说是类似于inode的数据结构，需要防止被修改。读锁已经足够用来防止父目录被删除。

这种锁模式的一个好处就是它允许对相同目录的并发变更操作。比如多个文件的创建可以在相同目录下并发创建：

- 每个获得该目录的一个读锁，以及文件的一个写锁。
- 目录名称上的读锁足够可以防止目录被删除，重命名或者快照。
- 文件名称上的写锁将会保证重复创建相同名称的文件的操作只会被执行一次。

因为名字空间有很多节点，所以读写锁对象只有在需要时才会被分配，一旦不再使用用就删除。为了避免死锁，锁是按照一个一致的全序关系进行获取的：

- 首先根据所处的名字空间树的级别
- 相同级别的则根据字典序。

### 4.2 备份放置
GFS在多个层次上都具有高度的分布式。它拥有数百个散布在多个机柜中的 chunkserver。这些 chunkserver 又可以被来自不同或者相同机柜上的 client 访问。处在不同机柜的机器间的通信可能需要穿过一个或者更多的网络交换机。此外，进出一个机柜的带宽可能会小于机柜内所有机器的带宽总和。多级的分布式带来了数据分布式时的扩展性，可靠性和可用性方面的挑战。

Chunk 的备份放置策略服务于两个目的：

- 最大化数据可靠性和可用性
- 最小化网络带宽的使用

为了达到这两个目的，仅仅将备份放在不同的机器是不够的，这只能应对机器或者硬盘失败，以及最大化利用每台机器的带宽。我们必须在机柜间存放备份。这样能够保证当一个机柜整个损坏或者离线(比如网络交换机故障或者电路出问题)时，该chunk的存放在其他机柜的某些副本仍然是可用的。这也意味着对于一个 chunk 的流量，尤其是读取操作可以充分利用多个机柜的带宽。另一方面，写操作需要在多个机柜间进行，但这是我们可以接受的。

###  4.3 创建/重备份/重平衡
Chunk 副本的创建主要有三个原因：chunk 的创建，重备份，重平衡。

当 master 创建一个 chunk 时，它将初始化的空的副本放置在何处。它会考虑几个因素：

1. 尽量把新的 chunk 放在那些低于平均磁盘空间使用值的那些 chunkserver 上。随着时间的推移，这会使得 chunkserver 的磁盘使用趋于相同
2. 尽量限制每个 chunkserver 上的最近的文件创建数，虽然创建操作是很简单的，但是它后面往往跟着繁重的写操作，因为chunk 的创建通常是因为写者的需要而创建它。在我们的一次 append 多次读的工作负载类型中，一旦写入完成，它们就会变成只读的。
3. 希望在机柜间存放 chunk 的副本

当 chunk 的可用备份数低于用户设定的目标值时，Master会进行重复制。有多个可能的原因导致它的发生：

- chunkserver 不可用
- chunkserver 报告它的某个备份已被污染
- 一块硬盘由于错误而不可用或者用户设定的目标值变大了

需要重复制的 chunk 根据几个因素确定优先级。

- 一个因素是它与备份数的目标值差了多少，比如给那些丢失了2个副本的 chunk 比丢失了 1 个的更高的优先级。
- 比起最近被删除的文件的 chunk，更想备份那些仍然存在的文件的 chunk(参考4.4节)。
- 为了最小化失败对于运行中的应用程序的影响，提高那些阻塞了用户进度的 chunk 的优先级。

Master 选择最高优先级的 chunk，通过给某个 chunkserver 发送指令告诉它直接从一个现有合法部分中拷贝数据来进行克隆。新备份的放置与创建具有类似的目标：

- 平均磁盘使用
- 限制在单个 chunkserver 上进行的 clone 操作数
- 使副本存放在不同机柜间

为了防止 clone 的流量淹没 client 的流量，master 限制整个集群已经每个 chunkserver 上处在活动状态的 clone 操作数。另外每个 chunkserver 还会限制它用在 clone 操作上的带宽，通过控制它对源 chunkserver 的读请求。

最后，master 会周期性的对副本进行重平衡。它检查当前的副本分布，然后为了更好的磁盘空间使用和负载瓶颈，将副本进行移动。而且在这个过程中，master 是逐步填充一个新的 chunkserver，而不是立即将新的 chunk 以及大量沉重的写流量使他忙的不可开交。对于一个新副本的放置，类似于前面的那些讨论。另外，master 必须选择删除哪个现有的副本。通常来说，它更喜欢那些存放在低于平均磁盘空闲率的 chunkserver 上的 chunk，这样可以使磁盘使用趋于相等。

### 4.4垃圾回收
文件删除后，GFS 并不立即释放可用的物理存储。它会将这项工作推迟到文件和 chunk 级别的垃圾回收时做。我们发现，这种方法使得系统更简单更可靠。
#### 4.4.1机制
当文件被应用程序删除时，master 会将这个删除操作像其他变化一样理解写入日志。文件不会被立即删除，而是被重命名为一个包含删除时间戳的隐藏名称。在 master 对文件系统进行常规扫描时，它会删除那些存在时间超过3天(这个时间是可以配置的)的隐藏文件。在此之前，文件依然可以用那个新的特殊名称进行读取，或者重命名回原来的名称来取消删除。当隐藏文件从名字空间删除后，它的元数据会被擦除。这样就有效地切断了它与所有 chunk 的关联。

在 chunk 的类似的常规扫描中，master 找到那些孤儿块(无法从任何文件到达)，擦除这些块的元数据。在与 master 周期交互的心跳信息中，chunkserver 报告它所拥有的 chunk 的那个子集，然后 master 返回那些不在 master 的元数据中出现的chunk 的标识。Chunkserver 就可以自由的删除这些 chunk 的那些副本了。
#### 4.4.2讨论
尽管程序设计语言中的分布式垃圾回收是一个需要复杂解决方案的难解问题，但是在这里它是很简单的。

- 我们可以简单的找到对于 chunk 的所有引用，因为它们保存在只由 master 维护的一个文件与 chunk 的映射里。
- 我们可以找到所有 chunk 的副本，它们不过是存放在每个 chunkserver 的特定目录下的 linux 文件。任何 master 不知道的副本就是垃圾。

采用垃圾回收方法收回存储空间与直接删除相比，提供了几个优势：

1. 在经常出现组件失败的大规模分布式系统中，它是简单而且可靠的。Chunk 创建可能在某些 chunkserver 上成功，在另外一些失败，这样就留下一些 master 所不知道的副本。副本删除消息可能丢失，master 必须记得在出现失败时进行重发。垃圾回收提供了一种同一的可信赖的清除无用副本的方式。
2. 它将存储空间回收与 master 常规的后台活动结合在一起，比如名字空间扫描，与 chunkserver 的握手。因此它们是绑在一块执行的，这样开销会被平摊。而且只有当 master 相对空闲时才会执行。Master 就可以为那些具有时间敏感性的客户端请求提供更好的响应。
3. 空间回收的延迟为意外的不可逆转的删除提供了一道保护网。

根据经验，主要的缺点是，当磁盘空间很紧张时，这种延时会妨碍到用户对磁盘使用的调整。那些频繁创建和删除中间文件的应用程序不能够立即重用磁盘空间。我们通过当已删除的文件被再次删除时加速它的存储回收来解决这个问题。我们也允许用户在不同的名字空间内使用不同的重备份和回收策略。比如用户可以指定某个目录树下的文件的 chunk 使用无副本存储，任何已经删除的文件会被立即删除并且从当前文件系统中彻底删除。

#### 4.5过期副本检测
如果 chunkserver 失败或者在它停机期间丢失了某些更新，chunk 副本就可能变为过期的。对于每个 chunk，master 维护一个版本号来区分最新和过期的副本。

只要 master 为一个 chunk 授权一个新的租约，版本号就会增加，然后通知副本进行更新。在一致的状态下，Master 和所有副本都会记录这个新的版本号。这发生在任何 client 被通知以前，因此也就是 client 开始向 chunk 中写数据之前。如果另一个副本当前不可用，它的 chunk 版本号就不会被更新。当 chunkserver 重启或者报告它的 chunk 和对应的版本号的时候，master 会检测该 chunkserver 是否包含过期副本。如果 master 发现有些版本号大于它的记录，master 就认为它在授权租约时失败了，所以采用更高的版本号的那个进行更新。

Master 通过周期性的垃圾回收删除过期副本。在此之前，对于客户端对于该 chunk 的请求 master 会直接将过期副本当作根本不存在进行处理。作为另外一种保护措施，当 master 通知客户端那个 chunkserver 包含某 chunk 的租约或者当它在 clone 操作中让 chunkserver 从另一个 chunkserver 中读取 chunk 时，会将 chunk 的版本号包含在内。当 clinet 和 chunkserver 执行操作时，总是会验证版本号，这样就使得它们总是访问最新的数据。
## 5.容错和诊断
在设计系统时，一个最大的挑战就是频繁的组件失败。组件的数量和质量使得这些问题变成一种常态而不再是异常。我们不能完全信任机器也不能完全信任磁盘。组件失败会导致系统不可用，甚至是损坏数据。我们讨论下如何面对这些挑战，以及当它们不可避免的发生时，在系统中建立起哪些工具来诊断问题。
### 5.1高可用性
在GFS的数百台服务器中，在任何时间总是有一些是不可用的。我们通过两个简单有效的策略来保持整个系统的高可用性：快速恢复和备份
#### 5.1.1快速恢复
Master 和 chunkserver 都设计得无论怎么样地被终止，都可以在在几秒内恢复它们的状态并启动。事实上，我们并没有区分正常和异常的终止。服务器通常都是通过杀死进程来关闭。客户端和其他服务器的请求超时后会经历一个小的停顿，然后重连那个重启后的服务器，进行重试。6.2.2 报告了观测到的启动时间。
#### 5.1.2 chunk 备份
正如之前讨论的，每个 chunk 备份在不同机柜上的多个 chunkserver 上。用户可以在不同名字空间内设置不同的备份级别，默认是3.当 chunkserver 离线或者通过检验和检测到某个 chunk 损坏后(5.2节)，master 会克隆现有的副本使得副本的数保持充足。尽管副本已经很好的满足了我们的需求，我们还探寻一些其他的具有同等或者更少 code 的跨机器的冗余方案，来满足我们日益增长的只读存储需求。我们期望在我们的非常松散耦合的系统中实现这些更复杂的冗余模式是具有挑战性但是可管理的。因为我们的负载主要是 append 和读操作而不是小的随机写操作。
#### 5.1.3 master 备份
为了可靠性，master 的状态需要进行备份。它的操作日志和检查点备份在多台机器上。对于状态的变更只有当它的操作日志被写入到本地磁盘和所有的远程备份后，才认为它完成。为了简单起见，master 除了负责进行各种后台活动比如：垃圾回收外，还要负责处理所有的变更。当它失败后，几乎可以立即重启。如果它所在的机器或者硬盘坏了，独立于GFS的监控设施会利用备份的操作日志在别处重启一个新的 master 进程。Client 仅仅使用 master 的一个典型名称(比如gfs-test)来访问它，这是一个 DNS 名称，如果master 被重新部署到一个新的机器上，可以改变它。

此外，当主 master down 掉之后，还有多个影子 master 可以提供对文件系统的只读访问。它们是影子，而不是镜像，这意味着它们可能比主 master 要滞后一些，通常可能是几秒。对于那些很少发生变更的文件或者不在意轻微过时的应用程序来说，它们增强了读操作的可用性。实际上，因为文件内容是从 chunkserver 中读取的，应用程序并不会看到过期的文件内容。文件元数据可能在短期内是过期的，比如目录内容或者访问控制信息。

为了保持自己的实时性，影子服务器会读取不断增长的操作日志的副本，然后像主 master 那样将这些变化序列应用在自己的数据结构上。与主 master 一样，它也会在启动时向 chunkserver 拉数据来定位 chunk 的副本，也会同它们交换握手信息以监控它们的状态。只有在主 master 决定创建或者删除副本时引起副本位置信息更新时，它才依赖于主 master。
### 5.2数据完整性
每个 chunkserver 通过`检验和`检测存储数据中的损坏。GFS集群通常具有分布在几百台机器上的数千块硬盘，这样它就会经常出现导致数据损坏或丢失的硬盘失败。我们可以从 chunk 的其他副本中恢复被损坏的数据，但是如果通过在 chunkserver 间比较数据来检测数据损坏是不现实的。另外，有分歧的备份仍然可能是合法的：根据GFS的变更语义，尤其是前面提到的原子性的 record append 操作，并不保证所有副本是完全一致的。因此每个 chunkserver 必须通过维护一个`检验和`来独立的验证它自己的拷贝的完整性。

一个 chunk 被划分为 64kb 大小的块。每个块有一个相应的 32bit 的`校验和`。与其他的元数据一样，`校验和`与用户数据分离的，它被存放在内存中，同时通过日志进行持久化存储。

对于读操作

- chunkserver 在向请求者(可能是 client 或者其他 chunkserver )返回数据前，需要检验与读取边界重叠的那些数据库的`校验和`。因此 chunkserver 不会将损坏数据传播到其他机器上去。
- 如果一个块的`校验和`与记录中的不一致，chunkserver 会向请求者返回一个错误，同时向 master 报告这个不匹配。
- 之后，请求者会向其他副本读取数据，而 master 则会用其他副本来 clone 这个 chunk。
- 当这个合法的新副本创建成功后，master 向报告不匹配的那个 chunkserver 发送指令删除它的副本。

`校验和`对于读性能的影响很小，因为：我们大部分的读操作至少跨越多个块，我们只需要读取相对少的额外数据来进行验证。GFS client 代码通过尽量在校验边界上对齐读操作大大降低了开销。另外在 chunkserver 上`校验和`的查找和比较不需要任何的 IO 操作，`校验和` 的计算也可以与IO操作重叠进行。

`校验和`计算对于 append 文件末尾的写操作进行了特别的优化。因为它们在工作负载中占据了统治地位。我们仅仅增量性的更新最后一个校验块的校验值，同时为那些 append 尾部的全新的校验块计算它的校验值。即使最后一个部分的校验块已经损坏，而我们现在无法检测出它，那么新计算出来的校验和将不会与存储数据匹配，那么当这个块下次被读取时，就可以检测到这个损坏。(

- 这里并没有验证最后一个块的校验值，而只是更新它的值
- 这里省去了验证的过程

举个例子假设最后一个校验块出现了错误，由于我们的校验值计算时是增量性的，也就是说下次计算不会重新计算已存在的这部分数据的`校验和`，这样该损坏就继续保留在`校验和`里，关键是因为这里采用了增量型的校验和计算方式

与之相对的，如果一个写操作者覆盖了一个现有 chunk 的边界，我们必须首先读取和验证操作边界上的第一个和最后一个块，然后执行写操作，最后计算和记录新的`校验和`。如果在覆盖它们之前不验证第一个和最后一个块，新的校验和就可能隐藏掉那些未被覆盖的区域的数据损坏。(因为这里没有采用增量计算方式，它是覆盖不是 append 所以现有的`检验和`就是整个块的没法从中取出部分数据的`校验和`，必须重新计算)。

在空闲期间，chunkserver 可以扫描验证处在非活动状态的 trunk 的内容。这允许我们检测到那些很少被读取的数据的损失。一旦损坏被发现，master 就可以创建一个新的未损坏副本并且删除损坏的副本。这就避免了一个不活跃的坏块骗过 master。 
### 5.3 诊断工具
全面而详细的诊断性的日志以很小的成本，带来了在问题分解，调试，性能分析上不可估量的帮助。没有日志，就很难理解那些机器间偶然出现的不可重复的交互。GFS 生成一个诊断日志用来记录很多重要事件(比如 chunkserver 的启动停止)以及所有 RPC 请求和应答。这些诊断日志可以自由的删除而不影响系统的正常运行。然而，只要磁盘空间允许，我们会尽量保存这些日志。

RPC 日志包含了所有的请求和响应信息，除了读写的文件数据。通过匹配请求和响应，整理不同机器上的 RPC 日志，我们可以重新构建出整个交互历史来诊断一个问题。这些日志也可以用来进行负载测试和性能分析。

因为日志是顺序异步写的，因此写日志对于性能的影响是很小的，得到的好处却是大大的。最近的事件也会保存在内存中，可以用于持续的在线监控。
## 6.测量
用一些小规模的测试来展示GFS架构和实现固有的一些瓶颈，有一些数字来源于 google 的实际集群。
### 6.1小规模测试
我们在一个由一个master，两个 master 备份，16 个chunkserver，16 个client组成的 GFS 集群上进行了性能测量。这个配置是为了方便测试，实际中的集群通常会有数百个 chunkserver，数百个 client。

所有机器的配置是，双核 PIII 1.4GHz 处理器，2GB 内存，两个 80G，5400rpm 硬盘，以及 100Mbps 全双工以太网连接到HP2524 交换机。所有19个 GFS 服务器连接在一个交换机，所有 16 个客户端连接在另一个上。两个交换机用 1Gbps 的线路连接。

![](./pic/gfs4.png)
#### 6.1.1读操作
N个客户端从文件系统中并发读。每个客户端在一个 320GB 的文件集合里随机 4MB 进行读取。然后重复256次，这样每个客户端实际上读取了 1GB 数据。Chunkserver 总共只有 32GB 内存，估计在 linux 的buffer cache 里最多有 10% 的命中率。我们的结果应该很接近一个几乎无缓存的结果。

- 图3

![](./pic/gfs5.png)

图3(左边) 展示了对于N个客户端的总的读取速率以及它的理论上的极限。

- 2个交换机通过一个1Gbps的链路连接时，极限峰值是125MB/s
- 客户端通过 100Mbps 连接，那么换成单个客户端的极限就是12.5MB/s。
- 当只有一个客户端在读取时，观察到的读取速率是10MB/s，达到了单个客户端极限的80%。
- 当16个读取者时，总的读取速率的94 MB/s，大概达到了链路极限(125MB/s)的75%，换成单个客户端就是6 MB/s。效率从80%降到了75%，是因为伴随着读取者的增加，多个读者从同一个 chunkserver 并发读数据的概率也随之变大。

#### 6.1.2写操作
N个客户端并行向N个不同的文件写数据。每个客户端以 1MB 的单个写操作总共向一个新文件写入 1GB 数据。单个客户端的写入速率是 6.3 MB/s，大概是极限值的一半(12.5MB)。主要原因是网络协议栈不能充分利用 chunk 副本数据推送的流水线模式。将数据从一个副本传递到另一个副本的延迟降低了整体的写速率。这样总的写速率以及它的理论上的极限如图3(中间)所示。

- 极限值变成了67 MB/s，因为客户端是一半，总极限峰值也是 125MB/s 的一半
- 对于16个客户端，总体的写入速率达到了35 MB/s，平均每个客户端2.2 MB/s,大概是理论极限的一半。

与写操作类似，伴随着写者的增加，多个写者从同一个chunkserver并发写数据的概率也随之变大。另外对于16个写者比16个读者更容易产生碰撞，因为每个写者将关联到3个不同的副本。

 写者比我们期望的要慢。在实际中，这还末变成一个主要问题，因为尽管它可能增加单个客户端的延时，但是当系统面对大量客户端时，其总的写入带宽并没有显著的影响。
#### 6.1.3记录追加
图3(最右边)展示了 record append 的性能。N个客户端向单个文件并行的 append。性能取决于保存了该文件最后那个 chunk 的那些 chunkserver，与客户端的数目无关。

- 当只有一个客户端时，能达到6.0MB/s
- 当有16个客户端时就降到了4.8 MB/s。

主要是由于拥塞以及不同的客户端的网络传输速率不同造成的。

我们的应用程序倾向于并行创建多个这样的文件。换句话说，N个客户端向M个共享文件并行 append，在这里N和M通常是几十甚至几百大小。因此在我们的实验中出现的 chunkserver 的网络拥塞问题在实际中并不是一个显著的问题，因为当一个文件的chunkserver 比较繁忙的时候，它可以去写另一个。

### 6.2现实的集群
![](./pic/gfs6.png)
选择在google内部使用的两个集群进行测试作为相似的那些集群的一个代表。

- 集群A主要用于 100 多个工程的日常研发。它会从数TB的数据中读取数MB的数据，对这些数据进行转化或者分析，然后将结果再写回集群。
- 集群B主要用于产品数据处理。它上面的任务持续时间更长，持续地在生成和处理数TB的数据集合，只是偶尔可能需要人为的参与。

在这两种情况下，任务都是由分布在多个机器上的很进程组成，它们并行的读写很多文件。 
#### 6.2.1存储
正如表中前5个字段所展示的，两个集群都有数百个 chunkserver，支持TB级的硬盘空间，空间已经被充分使用但还没全满。已用的空间包含 chunk 的所有副本。通常文件存在三个副本，因此这两个集群实际分别存储了 18TB 和 52TB 的数据。 

这两个集群的文件数目很接近，尽管B集群有大量的死文件(那些已经被删除或者被新版本文件所替换但空间还没有被释放的文件)。而且它具有更多的 trunk ，因为它上面的文件通常更大。
#### 6.2.2元数据
所有的 Chunkserver 总共存储了数十G的元数据，大部分是用户数据的64kb块的`校验和`。Chunkserver 上唯一的其他的元数据就是4.5节讨论的 chunk 的版本号。

保存在 master 上的元数据要更小一些，只有数十MB，平均下来每个文件只有100来个字节。这也刚好符合我们的 master 的内存不会成为实际中系统容量限制的设想。每个文件的元数据主要是以前缀压缩格式存储的文件名称。还有一些其他的元数据比如文件所有者，权限，文件到 chunk 的映射以及 chunk 的当前版本。另外对于每个 chunk 我们还存储了当前的副本位置以及用于实现写时复制的引用计数。

每个独立的 server(chunkserver和master) 只有 50-100MB 的元数据。因此，恢复是很快的：在 server 可以应答查询前只需要花几秒钟的时间就可以把它们从硬盘上读出来。然而，master 的启动可能要慢一些，通常还需要 30-60 秒从所有的chunkserver 获得 chunk 的位置信息。
#### 6.2.3读写速率
![](./pic/gfs7.png)
表3展示了不同时期的读写速率。测量期两个集群都已经运行了大约一周(被重启过)。

从启动开始看，平均写速率小于 30MB/s。当测量时，集群B正在以 100MB/s 的速率进行密集的写操作，同时产生了300MB/s 的网络负载，因为写操作将会传给3个副本。

读速率要远高于写速率。正如预想的那样，整个工作负载组成中，读多于写。这两个集群都处在繁重的读活动中。尤其是，A已经在过去的一个星期中维持了580MB/s的读速率。它的网络配置可以支持750MB/s，因此它已经充分利用了资源。B集群可支持 1300 MB/s的峰值读速率，但是应用只使用了 380 MB/s。
#### 6.2.4 master负载
表3 也表明发送给 master 的操作速率大概是每秒 200-500 个操作。Master 可以轻易的处理这个级别的速率，因此对于这些工作负载来说，它不会成为瓶颈。

在早期版本的GFS中，master 偶尔会成为某些工作负载的瓶颈。为了查找文件，花费大量的时间在巨大的目录(包含上千万的文件)中进行线性扫描。因此，我们改变了master的数据结构，使之可以在名字空间内进行有效的二分搜索。现在它可以简单的支持每秒上千次的文件访问。如果必要的话，我们还可以进一步的在名字空间数据结构前端提供名字查找缓存。
#### 6.2.5 恢复时间
一台 Chunkserver 失败后，它上面的那些 chunk 的副本数就会降低，必须进行 clone 以维持正常的副本数。恢复这些 chunk 的时间取决于资源的数量。在一个实验中，我们关闭集群B中的一个 chunkserver。该 chunkserver 大概有 15000 个 chunk ，总共 600GB 的数据。为减少对于应用程序的影响以及为调度决策提供余地，我们的默认参数设置将集群的并发 clone 操作限制在 91 个(占 chunkserver 个数的40%),同时每个 clone 操作最多可以消耗 6.25MB/s(50Mbps)。所有的 chunk 在23.2 分钟内被恢复，备份速率是 440MB/s。

在另一个实验中，我们关掉了两个 chunkserver，每个具有 16000 个 chunk，660GB 的数据。这次失败使得 266 个 chunk 降低到了一个副本，但是两分钟内，它们就恢复到了至少 2 个副本，这样就让集群能够容忍另一个 chunkserver 发生失败，而不产生数据丢失。
### 6.3 工作负载剖析
继续在两个新的集群上对工作负载进行细致的对比分析。

- 集群X是用于研究开发的
- 集群Y是用于产品数据处理

#### 6.3.1 方法和说明
这些结果只包含了客户端产生的请求，因此它们反映了应用程序的对整个文件系统的工作负载。并不包含为了执行客户端的请求进行的server 间的请求，或者是内部的后台活动，比如写推送或者是重平衡。

对于IO操作的统计是从 GFS 的 server 的 PRC 请求日志中重新构建出来的。比如为了增加并行性，GFS 客户端代码可能将一个读操作拆分为多个 RPC 请求，我们通过它们推断出原始请求。因为我们的访问模式高度的程式化，希望每个错误都可以出现在日志中。应用程序显式的记录可以提供更精确的数据，但是重新编译以及重启正在运行中的客户端在逻辑上是不可能这样做的。而且由于机器数很多，收集这些数据也会变得很笨重。

需要注意的是，不能工作负载过于通用化。因为GFS和应用程序是由google完全控制的，应用程序都是针对GFS进行专门优化的，同时GFS也是专门为这些应用而设计的。这种相互的影响可能也存在于一般的文件系统及其应用程序中，但是这种影响可能并不像我们上面所描述的那样。
#### 6.3.2 chunkserver负载
#### 表4 根据操作根据大小的分布
![](./pic/gfs8.png)
读操作的大小表现出双峰分布，小型读操作(小于64kb)来自于那些在大量文件中查找小片数据的随机读客户端，大型读操作(超过512kb)来自于穿越整个文件的线性读操作。

- 读
	- 集群Y中大量的读操作没有返回数据。我们应用程序，尤其是在产品系统中，经常使用文件作为生产者消费者队列。生产者并行的往文件中 append 数据，而消费者则从文件尾部读数据。有时候，如果消费者超过了生产者，就没有数据返回。
	- 集群X很少出现这种情况，因为它主要是用来进行短期数据分析，而不是长期的分布式应用。
- 写

	写操作的大小也表现出双峰分布。大型的写操作(超过256KB)通常来自于写操作者的缓冲。那些缓冲更少数据的写操作者，检查点或者经常性的同步或者简单的数据生成组成了小型的写操作(低于64KB)。
- append

	对于记录的 append，Y集群比X集群可以看到更大的大 record append 比率。因为使用Y集群的产品系统，针对GFS进行了更多的优化。

#### 表5根据数据传输总量	
![](./pic/gfs9.png)	
对于各种操作来说，大型的操作(超过256KB)构成了大部分的数据传输。但是小型(低于64KB)的读操作虽然传输了比较少的数据但是在数据读中也占据了相当的一部分，主要是由于随机 seek 造成的。
#### 6.3.4 append与write
记录 append 操作被大量的应用尤其是在我们的产品系统中。

- 对于集群X来说
	- 按字节传输来算，write与append的比例是108：1
	- 根据操作数来算比例是8：1。
- 对于集群Y，比例变成了3.7：1和2.5：1。

对于这两个集群来说，它们的 append 操作都要比 write 操作大一些｛操作数的比要远大于字节数的比，说明单个的 append 操作的字节数要大于 write。对于集群X来说，在测量期间的记录 append 操作要低一些，这可能是由其中具有特殊缓冲大小设置的应用程序造成的。

正如期望的，数据变更操作处于支配地位的是追加而不是重写{write也可能是追加}。我们测量了在主副本上的数据重写数量。对于集群X来说，以字节大小计算的话重写大概占了整个数据变更的0.0001%，以操作个数计算，大概小于0.0003%。对于Y集群来说，这两个数字都是0.05%，尽管这也不算大，但是还是要高于我们的期望。结果显示，大部分的重写是由于错误或者超时导致的客户端重写而产生的。它们并不是工作负载的一部分，而是属于重试机制。
#### 6.3.4 master负载
![](./pic/gfs10.png)
表6展示了对于 master 各种请求类型的剖析。大部分请求是为了得到 chunk 位置以及数据变更需要的租约持有信息。

可以看到集群X和Y在 delete 请求上的限制区别，因为集群Y上存储的产品信息会周期性地生成被新版本数据所替换。这些不同被隐藏在 open 请求中，因为老版的数据在被写的时候的打开操作中被隐式的删除(类似与Unix的”w”打开模式)。

查找匹配文件是一个类似于ls的模式匹配请求。不像其他的请求，它可能需要处理很大部分的名字空间，因此可能是很昂贵的。在集群Y上可以更频繁地看到它，因为自动化的数据处理任务为了了解整个应用程序的状态可能需要检查文件系统中的某些部分。与此相比，集群X需要更多显式的用户控制而且已经提前知道所需要的文件的名称。
## 7.经验
起初，GFS只是考虑作为我们产品系统的后端文件系统。随着时间的推移，开始在研究和开发中使用。一开始它基本不支持像权限，quota这些东西，但是现在都已经有了。产品系统是很容易控制的，但是用户却不是。因此需要更多的设施来避免用户间的干扰。最大的问题是硬盘和linux相关性。

- 问题一 硬盘支持各种版本linux驱动问题

	硬盘声称支持各种IDE协议版本的linux驱动，但是实际上它们只能在最近的一些上才能可靠的工作。因此如果协议版本如果相差不大，硬盘大多数情况下都可以工作，但是有时候这种不一致会使得驱动和内核在硬盘状态上产生分歧。由于内核的问题，这将会导致数据被默默的污染。这个问题使得我们使用`校验和`来检测数据污染，如果出现这种情况，我们就需要修改内核来处理这种协议不一致的情况。

- 问题二 由于linux2.2内核的fsync()的时间花费问题也碰到过一些问题

	它的花费是与文件大小而不是被修改部分的大小相关的。这对于我们大的操作日志会是一个问题，尤其是在我们实现检查点之前。我们通过改用同步写来绕过了这个问题，最后迁移到 Linux2.4 最终解决了它。

- 问题三 由于linux产生的问题是与读写锁相关的

	在一个地址空间里的线程在从硬盘中读页数据(读锁)或者在mmap调用中修改地址空间(写锁)的时候，必须持有一个读写锁。在系统负载很高，产生资源瓶颈或者出现硬件失败时，我们碰到了瞬态的超时。最后，我们发现当磁盘读写线程处理前面映射的数据时，这个锁阻塞了网络线程将新的数据映射到内存。由于我们的工作瓶颈主要是在网络带宽而不是内存带宽，因此我们通过使用pread()加上额外的开销替代mmap()绕过了这个问题。

尽管出现了一些问题，linux代码的可用性帮助了我们探索和理解系统的行为。在适当的时机，我们也会改进内核并与开源社区共享这些变化。
## 8.相关工作
像其他的大型分布式文件系统比如AFS，GFS提供了一个本地的独立名字空间，使得数据可以为了容错或者负载平衡而透明的移动。但与AFS不同的是，为了提升整体的性能和容错能力，GFS将文件数据在多个存储服务器上存储，这点更类似于xFS或者Swift。

- 副本策略

	硬盘相对便宜，而且与复杂的RAID策略相比，副本策略更简单。由于GFS完全采用副本策略进行冗余因此比xFS或者Swift消耗更多的原始存储。
- 不要缓存的原因

	与AFS,xFS,Frangipani,Intermezzo这些系统相比，GFS在文件系统接口下并不提供任何缓存。我们的目标工作负载类型对于通常的单应用程序运行模式来说，基本上是不可重用的，因为这种模式通常需要读取大量数据集合或者在里面进行随机的 seek，而每次只读少量的数据。
- 中心化服务节点

	一些分布式文件系统比如xFS,Frangipani,Minnesota’s GFS和GPFS删除了中央服务节点，依赖于分布式的算法进行一致性和管理。我们选择中央化测量是为了简化设计增加可靠性，获取灵活性。尤其是，一个中央化的 master 更容易实现复杂的 chunk 放置和备份策略，因为 master 具有大部分的相关信息以及控制了它们的改变。我们通过让 master 状态很小以及在其他机器上进行备份来解决容错。当前通过影子 master 机制提供可扩展性和可用性。对于 master 状态的更新，通过 append 到 write-ahead 日志里进行持久化。因此我们可以通过类似于 Harp 里的主 copy 模式来提供一个比我们当前模式具有更强一致性的高可用性。
- 未来将解决类似于Lustre的一个问题

	大量客户端的整体性能。然而我们通过专注于我们自己的需求而不是构建一个POSIX兼容文件系统来简化了这个问题。另外，GFS加速不可靠组件的数量是很大的，因此容错是我们设计的中心。

GFS很类似于NASD架构。但是NASD是基于网络连接的硬盘驱动器，GFS则使用普通机器作为 chunkserver。与NASD不同，chunkserver 在需要时分配固定大小的 chunk，而没有使用变长对象。此外，GFS 还实现了诸如重平衡，副本，产品环境需要的快速恢复。

不像Minnesota’s GFS和NASD，我们并没有寻求改变存储设备的模型。我们更专注于解决使用现有商品化组件组成的复杂分布式系统的日常的数据处理需求。

通过在生产者消费者队列中使用原子 record append 操作解决了与分布式操作系统 River 的类似问题。River 使用基于内存的跨机器分布式队列以及小心的数据流控制来解决这个问题，而GFS只使用了一个可以被很多生产者 append 数据的文件。River 模型支持 m to n 的分布式队列，但是缺乏容错，GFS目前只支持 m to 1。多个消费者可以读取相同文件，但是它们必须协调好对输入负载进行划分(各自处理不相交的一部分)。
## 9.总结
GFS包含了那些在商品化硬件上支持大规模数据处理的必要特征。尽管某些设计决定与我们特殊的应用类型相关，但是可以应用在具有类似需求和特征的数据处理任务中。

针对我们当前的应用负载类型，我们重新审视传统的文件系统的一些假设。我们的审视，使得我们的设计中产生了一些与之根本不同的观点。我们将组件失败看做常态而不是异常，为经常进行的在大文件上的append进行优化，然后是读(通常是顺序的)，为了改进整个系统我们扩展并且放松了标准文件系统接口。

我们的系统通过监控，备份关键数据，快速和自动恢复来提供容错。Chunk备份使得我们可以容忍chunkserver的失败。这些经常性的失败，驱动了一个优雅的在线修复机制的产生，它周期性地透明的进行修复尽快的恢复那些丢失的副本。另外，我们通过使用`校验和`来检测数据损坏，当系统中硬盘数目很大的时候，这种损坏变得很正常。

我们的设计实现了对于很多执行大量任务的并发读者和写者的高吞吐率。通过从数据传输中分离文件系统控制，我们来实现这个目标，让master来处理文件系统控制，数据传输则直接在chunkserver和客户端之间进行。通过增大chunk的大小以及chunk的租约机制，降低了master在普通操作中的参与。这使中央的master不会成为瓶颈。我们相信在当前网络协议栈上的改进将会提供客户端写出速率的限制。

GFS成功地满足了我们的存储需求，同时除了作为产品数据处理平台外，还作为研发的存储平台而被广泛使用。它是一个使我们可以持续创新以及面对整个web的海量数据挑战的重要工具。











